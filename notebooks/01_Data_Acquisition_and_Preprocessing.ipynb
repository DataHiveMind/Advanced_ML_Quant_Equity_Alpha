{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e270fc4c",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "- To gather raw financial data from various sources, perform initial cleaning, handle missing values, and transform it into a consistent, usable format ready for feature engineering. This notebook establishes the foundation of your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2347df3",
   "metadata": {},
   "source": [
    "# Environment Setup & Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f12700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_libraries():\n",
    "    \"\"\"\n",
    "    Install required libraries using pip.\n",
    "    \"\"\"\n",
    "    required = [\"pandas\", \"numpy\", \"yfinance\", \"pyyaml\"]\n",
    "    for pkg in required:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "def setup_data_paths():\n",
    "    \"\"\"\n",
    "    Define and create data storage directories.\n",
    "    \"\"\"\n",
    "    raw_path = \"data/raw\"\n",
    "    processed_path = \"data/processed\"\n",
    "    os.makedirs(raw_path, exist_ok=True)\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    return raw_path, processed_path\n",
    "\n",
    "\n",
    "def load_config(config_path=\"config/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install_libraries()\n",
    "    raw_path, processed_path = setup_data_paths()\n",
    "    config = load_config()\n",
    "    print(f\"Config loaded: {config}\")\n",
    "    print(f\"Raw data path: {raw_path}, Processed data path: {processed_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83285fea",
   "metadata": {},
   "source": [
    "# Raw Data Acquisition (Simulated/Proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f861bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_yfinance_data(tickers, start, end, save_dir):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV and adjusted prices for given tickers using yfinance.\n",
    "    Save each ticker's data as a CSV in save_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for ticker in tickers:\n",
    "        df = yf.download(ticker, start=start, end=end, auto_adjust=False)\n",
    "        if df is not None and not df.empty:\n",
    "            df.to_csv(os.path.join(save_dir, f\"{ticker}_ohlcv.csv\"))\n",
    "            print(f\"Saved {ticker} data to {save_dir}\")\n",
    "        else:\n",
    "            print(f\"No data found for {ticker} in the given date range.\")\n",
    "\n",
    "def generate_mock_index_constituents(index_name, tickers, save_dir):\n",
    "    \"\"\"\n",
    "    Generate mock index constituent data.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df = pd.DataFrame({\n",
    "        \"index\": [index_name] * len(tickers),\n",
    "        \"ticker\": tickers,\n",
    "        \"weight\": np.round(np.random.dirichlet(np.ones(len(tickers))), 4)\n",
    "    })\n",
    "    df.to_csv(os.path.join(save_dir, f\"{index_name}_constituents.csv\"), index=False)\n",
    "    print(f\"Saved {index_name} constituents to {save_dir}\")\n",
    "\n",
    "def generate_mock_etf_flows(etf_names, dates, save_dir):\n",
    "    \"\"\"\n",
    "    Generate mock ETF flow data.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    data = []\n",
    "    for etf in etf_names:\n",
    "        flows = np.random.normal(0, 10_000_000, size=len(dates))\n",
    "        data.append(pd.DataFrame({\n",
    "            \"date\": dates,\n",
    "            \"etf\": etf,\n",
    "            \"flow_usd\": flows\n",
    "        }))\n",
    "    df = pd.concat(data, ignore_index=True)\n",
    "    df.to_csv(os.path.join(save_dir, \"etf_flows.csv\"), index=False)\n",
    "    print(f\"Saved ETF flows to {save_dir}\")\n",
    "\n",
    "def generate_mock_corporate_actions(tickers, dates, save_dir):\n",
    "    \"\"\"\n",
    "    Generate mock corporate actions (splits, dividends).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    actions = []\n",
    "    for ticker in tickers:\n",
    "        for date in np.random.choice(dates, size=2, replace=False):\n",
    "            actions.append({\n",
    "                \"ticker\": ticker,\n",
    "                \"date\": date,\n",
    "                \"action\": \"split\" if np.random.rand() > 0.5 else \"dividend\",\n",
    "                \"value\": np.round(np.random.uniform(0.1, 2.0), 2)\n",
    "            })\n",
    "    df = pd.DataFrame(actions)\n",
    "    df.to_csv(os.path.join(save_dir, \"corporate_actions.csv\"), index=False)\n",
    "    print(f\"Saved corporate actions to {save_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example tickers and dates\n",
    "    tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"]\n",
    "    etfs = [\"SPY\", \"IVV\", \"EFA\"]\n",
    "    indices = [(\"MSCI\", tickers), (\"S&P\", tickers), (\"FTSE\", tickers)]\n",
    "    start_date = (datetime.today() - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "    end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"B\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    raw_dir = \"data/raw\"\n",
    "\n",
    "    # Fetch historical market data\n",
    "    fetch_yfinance_data(tickers, start_date, end_date, raw_dir)\n",
    "\n",
    "    # Generate mock index constituents\n",
    "    for index_name, index_tickers in indices:\n",
    "        generate_mock_index_constituents(index_name, index_tickers, raw_dir)\n",
    "\n",
    "    # Generate mock ETF flows\n",
    "    generate_mock_etf_flows(etfs, dates, raw_dir)\n",
    "\n",
    "    # Generate mock corporate actions\n",
    "    generate_mock_corporate_actions(tickers, dates, raw_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185deeaa",
   "metadata": {},
   "source": [
    "# Initial Data Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def clean_dataframe(df, date_cols=['date'], price_cols=['open', 'high', 'low', 'close', 'adj_close'], tz='UTC'):\n",
    "    \"\"\"\n",
    "    Clean a DataFrame:\n",
    "    - Standardize column names\n",
    "    - Convert date columns to UTC datetime\n",
    "    - Convert price columns to numeric\n",
    "    - Fill missing values\n",
    "    \"\"\"\n",
    "    # Standardize column names\n",
    "    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "    # Convert date columns to datetime and localize to UTC\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "            if df[col].dt.tz is None:\n",
    "                df[col] = df[col].dt.tz_localize(tz)\n",
    "            else:\n",
    "                df[col] = df[col].dt.tz_convert(tz)\n",
    "\n",
    "    # Convert price columns to numeric\n",
    "    for col in price_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Sort by date and fill missing values\n",
    "    if date_cols and date_cols[0] in df.columns:\n",
    "        df = df.sort_values(by=date_cols[0])\n",
    "    df = df.ffill().bfill()\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_all_raw_data(raw_data_dir=\"data/raw\", cleaned_data_dir=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Load all CSV files from raw_data_dir using load_csv_file, clean, and save to cleaned_data_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(cleaned_data_dir, exist_ok=True)\n",
    "    for fname in os.listdir(raw_data_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(raw_data_dir, fname)\n",
    "            df = load_csv_file(fpath)\n",
    "            cleaned_df = clean_dataframe(df)\n",
    "            cleaned_path = os.path.join(cleaned_data_dir, fname)\n",
    "            cleaned_df.to_csv(cleaned_path, index=False)\n",
    "            print(f\"Cleaned and saved: {cleaned_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_all_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704b0ac",
   "metadata": {},
   "source": [
    "# Data Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def check_for_outliers(df, price_cols=['open', 'high', 'low', 'close', 'adj_close'], volume_col='volume', z_thresh=6):\n",
    "    \"\"\"\n",
    "    Identify extreme outliers in price and volume columns using z-score.\n",
    "    \"\"\"\n",
    "    outlier_report = {}\n",
    "    for col in price_cols + [volume_col]:\n",
    "        if col in df.columns:\n",
    "            vals = df[col].dropna()\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            z_scores = np.abs((vals - vals.mean()) / (vals.std() + 1e-8))\n",
    "            outliers = df[z_scores > z_thresh]\n",
    "            outlier_report[col] = {\n",
    "                \"num_outliers\": outliers.shape[0],\n",
    "                \"outlier_indices\": outliers.index.tolist()\n",
    "            }\n",
    "    return outlier_report\n",
    "\n",
    "def check_for_negatives(df, price_cols=['open', 'high', 'low', 'close', 'adj_close'], volume_col='volume'):\n",
    "    \"\"\"\n",
    "    Check for negative prices or volumes.\n",
    "    \"\"\"\n",
    "    negatives = {}\n",
    "    for col in price_cols + [volume_col]:\n",
    "        if col in df.columns:\n",
    "            neg = df[df[col] < 0]\n",
    "            negatives[col] = {\n",
    "                \"num_negative\": neg.shape[0],\n",
    "                \"negative_indices\": neg.index.tolist()\n",
    "            }\n",
    "    return negatives\n",
    "\n",
    "def check_for_gaps(df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Check for unexpected gaps in the date column (assuming business day frequency).\n",
    "    \"\"\"\n",
    "    if date_col not in df.columns:\n",
    "        return {\"gap_count\": None, \"gap_dates\": []}\n",
    "    df = df.sort_values(date_col)\n",
    "    dates = pd.to_datetime(df[date_col])\n",
    "    expected = pd.date_range(dates.min(), dates.max(), freq='B')\n",
    "    missing = set(expected) - set(dates)\n",
    "    return {\n",
    "        \"gap_count\": len(missing),\n",
    "        \"gap_dates\": sorted(list(missing))\n",
    "    }\n",
    "\n",
    "def summarize_coverage(df, ticker_col='ticker', date_col='date'):\n",
    "    \"\"\"\n",
    "    Summarize data coverage: start/end dates, number of unique tickers.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    if date_col in df.columns:\n",
    "        dates = pd.to_datetime(df[date_col])\n",
    "        summary['start_date'] = str(dates.min())\n",
    "        summary['end_date'] = str(dates.max())\n",
    "    if ticker_col in df.columns:\n",
    "        summary['num_unique_tickers'] = df[ticker_col].nunique()\n",
    "        summary['unique_tickers'] = df[ticker_col].unique().tolist()\n",
    "    return summary\n",
    "\n",
    "def run_integrity_checks_on_dir(data_dir=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Run integrity checks on all CSVs in a directory.\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            df = load_csv_file(fpath)\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            outliers = check_for_outliers(df)\n",
    "            negatives = check_for_negatives(df)\n",
    "            gaps = check_for_gaps(df)\n",
    "            coverage = summarize_coverage(df)\n",
    "            print(\"Outliers:\", outliers)\n",
    "            print(\"Negatives:\", negatives)\n",
    "            print(\"Gaps:\", gaps)\n",
    "            print(\"Coverage:\", coverage)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_integrity_checks_on_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92fcaa",
   "metadata": {},
   "source": [
    "# Save Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def save_all_csvs_as_parquet(processed_csv_dir=\"data/processed\", parquet_dir=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Load all CSV files from processed_csv_dir and save as Parquet files in parquet_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(parquet_dir, exist_ok=True)\n",
    "    for fname in os.listdir(processed_csv_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(processed_csv_dir, fname)\n",
    "            df = load_csv_file(fpath)\n",
    "            parquet_fname = fname.replace(\".csv\", \".parquet\")\n",
    "            parquet_path = os.path.join(parquet_dir, parquet_fname)\n",
    "            df.to_parquet(parquet_path, index=False)\n",
    "            print(f\"Saved {parquet_path}\")\n",
    "if __name__ == \"__main__\":\n",
    "    save_all_csvs_as_parquet()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
