{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5430ff",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "- To comprehensively evaluate the performance and robustness of your trading strategy, identifying strengths, weaknesses, and areas for improvement. This involves analyzing both deterministic and Monte Carlo backtest results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e891e48",
   "metadata": {},
   "source": [
    "# Load Backtest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_deterministic_backtest_results(reports_dir=\"results/backtest_reports\"):\n",
    "    \"\"\"\n",
    "    Load deterministic equity curve, trade log, and position log from the specified directory.\n",
    "    \"\"\"\n",
    "    equity_curve = pd.read_csv(os.path.join(reports_dir, \"equity_curve.csv\"))\n",
    "    trade_log = pd.read_csv(os.path.join(reports_dir, \"trade_log.csv\"))\n",
    "    position_log = pd.read_csv(os.path.join(reports_dir, \"position_log.csv\"))\n",
    "    return equity_curve, trade_log, position_log\n",
    "\n",
    "def load_monte_carlo_results(mc_dir=\"results/monte_carlo_sims\"):\n",
    "    \"\"\"\n",
    "    Load aggregated Monte Carlo metrics and all equity curves from the specified directory.\n",
    "    \"\"\"\n",
    "    metrics_df = pd.read_csv(os.path.join(mc_dir, \"monte_carlo_metrics.csv\"))\n",
    "    # Load all equity curves\n",
    "    equity_curves = []\n",
    "    for fname in os.listdir(mc_dir):\n",
    "        if fname.startswith(\"equity_curve_\") and fname.endswith(\".csv\"):\n",
    "            eq_curve = pd.read_csv(os.path.join(mc_dir, fname))\n",
    "            equity_curves.append(eq_curve)\n",
    "    return metrics_df, equity_curves\n",
    "\n",
    "# Example usage:\n",
    "# equity_curve, trade_log, position_log = load_deterministic_backtest_results()\n",
    "# mc_metrics,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def calculate_performance_metrics(equity_curve, freq='D', risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics for a deterministic backtest.\n",
    "    equity_curve: pd.Series or DataFrame with 'portfolio_value' and 'date' columns.\n",
    "    freq: 'D' for daily, 'W' for weekly, etc.\n",
    "    risk_free_rate: annual risk-free rate (as decimal).\n",
    "    Returns: dict of metrics.\n",
    "    \"\"\"\n",
    "    if isinstance(equity_curve, pd.DataFrame):\n",
    "        if 'portfolio_value' in equity_curve.columns:\n",
    "            values = equity_curve['portfolio_value'].values\n",
    "        else:\n",
    "            raise ValueError(\"equity_curve DataFrame must have 'portfolio_value' column.\")\n",
    "        if 'date' in equity_curve.columns:\n",
    "            dates = pd.to_datetime(equity_curve['date'])\n",
    "        else:\n",
    "            dates = pd.RangeIndex(len(values))\n",
    "    else:\n",
    "        values = equity_curve.values\n",
    "        dates = equity_curve.index\n",
    "\n",
    "    returns = np.diff(values) / values[:-1]\n",
    "    returns = np.insert(returns, 0, 0)  # First return is zero\n",
    "\n",
    "    # Time calculations\n",
    "    n_years = (dates[-1] - dates[0]).days / 365.25 if hasattr(dates, '__getitem__') else len(values) / 252\n",
    "    periods_per_year = {'D': 252, 'W': 52, 'M': 12}.get(freq, 252)\n",
    "\n",
    "    # Return metrics\n",
    "    total_return = values[-1] / values[0] - 1\n",
    "    cagr = (values[-1] / values[0]) ** (1 / n_years) - 1 if n_years > 0 else np.nan\n",
    "    ann_return = np.mean(returns) * periods_per_year\n",
    "\n",
    "    # Risk metrics\n",
    "    ann_vol = np.std(returns) * np.sqrt(periods_per_year)\n",
    "    max_dd = np.min((values - np.maximum.accumulate(values)) / np.maximum.accumulate(values))\n",
    "    skewness = skew(returns)\n",
    "    kurt = kurtosis(returns)\n",
    "\n",
    "    # Risk-adjusted\n",
    "    sharpe = (np.mean(returns) - risk_free_rate / periods_per_year) / (np.std(returns) + 1e-8) * np.sqrt(periods_per_year)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    sortino = (np.mean(returns) - risk_free_rate / periods_per_year) / (np.std(downside_returns) + 1e-8) * np.sqrt(periods_per_year)\n",
    "    calmar = cagr / abs(max_dd) if max_dd != 0 else np.nan\n",
    "\n",
    "    # Trade-level metrics (if available)\n",
    "    win_rate = profit_factor = avg_win = avg_loss = np.nan\n",
    "    if 'trade_log' in equity_curve.columns:\n",
    "        trade_log = equity_curve['trade_log']\n",
    "        wins = trade_log[trade_log['pnl'] > 0]\n",
    "        losses = trade_log[trade_log['pnl'] < 0]\n",
    "        win_rate = len(wins) / (len(wins) + len(losses)) if (len(wins) + len(losses)) > 0 else np.nan\n",
    "        profit_factor = wins['pnl'].sum() / abs(losses['pnl'].sum()) if losses['pnl'].sum() != 0 else np.nan\n",
    "        avg_win = wins['pnl'].mean() if not wins.empty else np.nan\n",
    "        avg_loss = losses['pnl'].mean() if not losses.empty else np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"Total Return\": total_return,\n",
    "        \"CAGR\": cagr,\n",
    "        \"Annualized Return\": ann_return,\n",
    "        \"Annualized Volatility\": ann_vol,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "        \"Sortino Ratio\": sortino,\n",
    "        \"Calmar Ratio\": calmar,\n",
    "        \"Skewness\": skewness,\n",
    "        \"Kurtosis\": kurt,\n",
    "        \"Win Rate\": win_rate,\n",
    "        \"Profit Factor\": profit_factor,\n",
    "        \"Average Win\": avg_win,\n",
    "        \"Average Loss\": avg_loss\n",
    "    }\n",
    "    return metrics, returns, dates\n",
    "\n",
    "def plot_equity_curve(equity_curve):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if isinstance(equity_curve, pd.DataFrame) and 'date' in equity_curve.columns:\n",
    "        plt.plot(pd.to_datetime(equity_curve['date']), equity_curve['portfolio_value'])\n",
    "    else:\n",
    "        plt.plot(equity_curve)\n",
    "    plt.title(\"Equity Curve\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Portfolio Value\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pnl_histogram(returns, freq='D'):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.hist(returns, bins=50, alpha=0.7)\n",
    "    plt.title(f\"{freq}-level PnL Histogram\")\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_drawdown(equity_curve):\n",
    "    if isinstance(equity_curve, pd.DataFrame) and 'portfolio_value' in equity_curve.columns:\n",
    "        values = equity_curve['portfolio_value'].values\n",
    "        dates = pd.to_datetime(equity_curve['date'])\n",
    "    else:\n",
    "        values = equity_curve.values\n",
    "        dates = equity_curve.index\n",
    "    peak = np.maximum.accumulate(values)\n",
    "    drawdown = (values - peak) / peak\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(dates, drawdown, color='red')\n",
    "    plt.title(\"Drawdown Chart\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Drawdown\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    equity_curve = pd.read_csv(\"results/backtest_reports/equity_curve.csv\")\n",
    "    metrics, returns, dates = calculate_performance_metrics(equity_curve)\n",
    "    print(\"Performance Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    plot_equity_curve(equity_curve)\n",
    "    plot_pnl_histogram(returns)\n",
    "    plot_drawdown(equity_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa787c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_mc_metrics(metrics_df):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the distribution of Sharpe, Max Drawdown, and Total Return across Monte Carlo paths.\n",
    "    \"\"\"\n",
    "    # Histograms\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, metric in enumerate(['Sharpe Ratio', 'Max Drawdown', 'Total Return']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        sns.histplot(metrics_df[metric], bins=40, kde=True)\n",
    "        plt.title(f\"Distribution of {metric}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplots\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(data=metrics_df[['Sharpe Ratio', 'Max Drawdown', 'Total Return']])\n",
    "    plt.title(\"Boxplot of Key Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify worst-case scenarios\n",
    "    worst_idx = metrics_df['Sharpe Ratio'].idxmin()\n",
    "    worst_metrics = metrics_df.loc[worst_idx]\n",
    "    print(\"Worst-case scenario (lowest Sharpe):\")\n",
    "    print(worst_metrics)\n",
    "\n",
    "    # Return indices for further analysis\n",
    "    return worst_idx\n",
    "\n",
    "def plot_mc_equity_curves(equity_curves, metrics_df, num_curves=100):\n",
    "    \"\"\"\n",
    "    Plot average, median, 10th, and 90th percentile equity curves from Monte Carlo simulations.\n",
    "    \"\"\"\n",
    "    # Align all curves by length\n",
    "    min_len = min(len(eq['portfolio_value']) for eq in equity_curves)\n",
    "    aligned_curves = np.array([eq['portfolio_value'][:min_len] for eq in equity_curves])\n",
    "\n",
    "    avg_curve = np.mean(aligned_curves, axis=0)\n",
    "    median_curve = np.median(aligned_curves, axis=0)\n",
    "    p10_curve = np.percentile(aligned_curves, 10, axis=0)\n",
    "    p90_curve = np.percentile(aligned_curves, 90, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(avg_curve, label='Average')\n",
    "    plt.plot(median_curve, label='Median')\n",
    "    plt.plot(p10_curve, label='10th Percentile', linestyle='--')\n",
    "    plt.plot(p90_curve, label='90th Percentile', linestyle='--')\n",
    "    # Optionally plot a sample of individual paths\n",
    "    for i in np.random.choice(len(aligned_curves), min(num_curves, len(aligned_curves)), replace=False):\n",
    "        plt.plot(aligned_curves[i], color='gray', alpha=0.1)\n",
    "    plt.title(\"Monte Carlo Equity Curves\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Portfolio Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Monte Carlo results\n",
    "    metrics_df = pd.read_csv(\"results/monte_carlo_sims/monte_carlo_metrics.csv\")\n",
    "    equity_curves = []\n",
    "    import os\n",
    "    eq_dir = \"results/monte_carlo_sims\"\n",
    "    for fname in os.listdir(eq_dir):\n",
    "        if fname.startswith(\"equity_curve_\") and fname.endswith(\".csv\"):\n",
    "            eq = pd.read_csv(os.path.join(eq_dir, fname))\n",
    "            equity_curves.append(eq)\n",
    "    # Analyze metrics\n",
    "    worst_idx = analyze_mc_metrics(metrics_df)\n",
    "    # Plot equity curves\n",
    "    plot_mc_equity_curves(equity_curves, metrics_df)\n",
    "    # Optionally, analyze worst-case path\n",
    "    print(\"\\nWorst-case equity curve (lowest Sharpe):\")\n",
    "    print(equity_curves[worst_idx].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Deep Dive & Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef861ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_trade_stats(trade_log):\n",
    "    \"\"\"\n",
    "    Analyze trade frequency, average holding period, and PnL by trade type.\n",
    "    Assumes trade_log has columns: ['date', 'ticker', 'side', 'size', 'fill_price', 'slippage', 'pnl', 'event_type']\n",
    "    \"\"\"\n",
    "    trade_log['date'] = pd.to_datetime(trade_log['date'])\n",
    "    # Trade frequency\n",
    "    freq = trade_log.groupby('side').size().to_dict()\n",
    "    # Average holding period (if entry/exit pairs are available)\n",
    "    holding_periods = []\n",
    "    trade_log = trade_log.sort_values(['ticker', 'date'])\n",
    "    for ticker, group in trade_log.groupby('ticker'):\n",
    "        entries = group[group['side'] == 'buy']\n",
    "        exits = group[group['side'] == 'sell']\n",
    "        for _, entry in entries.iterrows():\n",
    "            exit = exits[exits['date'] > entry['date']].head(1)\n",
    "            if not exit.empty:\n",
    "                holding_periods.append((exit['date'].values[0] - entry['date']).astype('timedelta64[D]').astype(int))\n",
    "    avg_holding = np.mean(holding_periods) if holding_periods else np.nan\n",
    "    # PnL by trade type\n",
    "    pnl_by_side = trade_log.groupby('side')['pnl'].sum().to_dict()\n",
    "    pnl_by_event = trade_log.groupby('event_type')['pnl'].sum().to_dict() if 'event_type' in trade_log.columns else {}\n",
    "\n",
    "    print(\"Trade Frequency by Side:\", freq)\n",
    "    print(\"Average Holding Period (days):\", avg_holding)\n",
    "    print(\"PnL by Side:\", pnl_by_side)\n",
    "    print(\"PnL by Event Type:\", pnl_by_event)\n",
    "    return freq, avg_holding, pnl_by_side, pnl_by_event\n",
    "\n",
    "def feature_attribution(trade_log, features_df, feature_cols, pnl_col='pnl', regime_col=None):\n",
    "    \"\"\"\n",
    "    Rudimentary attribution: Which features contributed most to profitable trades?\n",
    "    Optionally, analyze by market regime.\n",
    "    \"\"\"\n",
    "    # Merge features at trade entry with trade log\n",
    "    merged = pd.merge(trade_log, features_df, on=['date', 'ticker'], how='left')\n",
    "    # Correlation of features with PnL\n",
    "    corrs = merged[feature_cols + [pnl_col]].corr()[pnl_col].drop(pnl_col)\n",
    "    print(\"Feature-PnL Correlations:\")\n",
    "    print(corrs.sort_values(ascending=False))\n",
    "    # Attribution by regime\n",
    "    if regime_col and regime_col in merged.columns:\n",
    "        for regime, group in merged.groupby(regime_col):\n",
    "            print(f\"\\nRegime: {regime}\")\n",
    "            rcorrs = group[feature_cols + [pnl_col]].corr()[pnl_col].drop(pnl_col)\n",
    "            print(rcorrs.sort_values(ascending=False))\n",
    "    return corrs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load logs and features\n",
    "    trade_log = pd.read_csv(\"results/backtest_reports/trade_log.csv\")\n",
    "    features_df = pd.read_csv(\"data/features/AAPL_ohlcv_features.csv\")  # Example, adjust as needed\n",
    "    feature_cols = [col for col in features_df.columns if \"momentum\" in col or \"volatility\" in col or \"volume\" in col]\n",
    "    # Analyze trade stats\n",
    "    analyze_trade_stats(trade_log)\n",
    "    # Attribution\n",
    "    feature_attribution(trade_log, features_df, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis & Stress Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1517e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [0.0001, 0.0005, 0.001, 0.002]\n",
    "thresholds = [0.005, 0.01, 0.02]\n",
    "results = []\n",
    "for cost in costs:\n",
    "    for thresh in thresholds:\n",
    "        # Update config\n",
    "        config['transaction_cost'] = cost\n",
    "        config['signal_threshold'] = thresh\n",
    "        # Run backtest (script or function)\n",
    "        metrics = run_backtest_with_config(config)\n",
    "        results.append({'cost': cost, 'threshold': thresh, **metrics})\n",
    "df = pd.DataFrame(results)\n",
    "# Plot sensitivity\n",
    "\n",
    "# Restrict to 2008-2009\n",
    "crisis_data = market_data[(market_data['date'] >= '2008-01-01') & (market_data['date'] <= '2009-12-31')]\n",
    "metrics = run_backtest_with_data(crisis_data, config)\n",
    "print(metrics)\n",
    "\n",
    "# Simulate flash crash\n",
    "shock_data = market_data.copy()\n",
    "shock_data.loc[shock_data['date'] == '2010-05-06', 'close'] *= 0.85  # 15% drop\n",
    "metrics = run_backtest_with_data(shock_data, config)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f2501",
   "metadata": {},
   "source": [
    "# Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jinja2 import Template\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def summarize_findings(deterministic_metrics, mc_metrics_df):\n",
    "    # Deterministic summary\n",
    "    det_summary = {\n",
    "        \"Total Return\": deterministic_metrics[\"Total Return\"],\n",
    "        \"CAGR\": deterministic_metrics[\"CAGR\"],\n",
    "        \"Sharpe Ratio\": deterministic_metrics[\"Sharpe Ratio\"],\n",
    "        \"Max Drawdown\": deterministic_metrics[\"Max Drawdown\"],\n",
    "        \"Calmar Ratio\": deterministic_metrics[\"Calmar Ratio\"]\n",
    "    }\n",
    "    # Monte Carlo summary\n",
    "    mc_summary = {\n",
    "        \"Sharpe Ratio (mean)\": mc_metrics_df[\"Sharpe Ratio\"].mean(),\n",
    "        \"Sharpe Ratio (10th pct)\": mc_metrics_df[\"Sharpe Ratio\"].quantile(0.1),\n",
    "        \"Sharpe Ratio (90th pct)\": mc_metrics_df[\"Sharpe Ratio\"].quantile(0.9),\n",
    "        \"Max Drawdown (mean)\": mc_metrics_df[\"Max Drawdown\"].mean(),\n",
    "        \"Total Return (mean)\": mc_metrics_df[\"Total Return\"].mean()\n",
    "    }\n",
    "    # Strengths & weaknesses\n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "    if det_summary[\"Sharpe Ratio\"] > 1 and mc_summary[\"Sharpe Ratio (10th pct)\"] > 0.5:\n",
    "        strengths.append(\"Consistent risk-adjusted returns across most scenarios.\")\n",
    "    if det_summary[\"Max Drawdown\"] > -0.2:\n",
    "        strengths.append(\"Controlled drawdowns in deterministic and most MC paths.\")\n",
    "    if mc_metrics_df[\"Sharpe Ratio\"].min() < 0:\n",
    "        weaknesses.append(\"Some Monte Carlo paths show negative Sharpe ratios (strategy can underperform in adverse conditions).\")\n",
    "    if det_summary[\"Calmar Ratio\"] < 1:\n",
    "        weaknesses.append(\"Calmar Ratio below 1 indicates risk of large drawdowns relative to return.\")\n",
    "    if mc_metrics_df[\"Max Drawdown\"].min() < -0.5:\n",
    "        weaknesses.append(\"Severe drawdowns possible in worst-case scenarios.\")\n",
    "\n",
    "    # Future research\n",
    "    future = [\n",
    "        \"Engineer new features (e.g., alternative data, regime indicators).\",\n",
    "        \"Experiment with advanced ML models (transformers, ensemble stacking).\",\n",
    "        \"Enhance risk controls (dynamic position sizing, stop-loss logic).\",\n",
    "        \"Improve execution simulation (market impact, order book modeling).\"\n",
    "    ]\n",
    "    return det_summary, mc_summary, strengths, weaknesses, future\n",
    "\n",
    "def generate_html_report(det_summary, mc_summary, strengths, weaknesses, future, output_path=\"results/final_performance_report.html\"):\n",
    "    template = Template(\"\"\"\n",
    "    <html>\n",
    "    <head><title>Final Performance Report</title></head>\n",
    "    <body>\n",
    "    <h1>Strategy Performance Report</h1>\n",
    "    <h2>1. Deterministic Backtest Summary</h2>\n",
    "    <ul>\n",
    "      {% for k, v in det_summary.items() %}\n",
    "        <li><b>{{k}}:</b> {{'%0.4f' % v if v is not None else 'N/A'}}</li>\n",
    "      {% endfor %}\n",
    "    </ul>\n",
    "    <h2>2. Monte Carlo Simulation Summary</h2>\n",
    "    <ul>\n",
    "      {% for k, v in mc_summary.items() %}\n",
    "        <li><b>{{k}}:</b> {{'%0.4f' % v if v is not None else 'N/A'}}</li>\n",
    "      {% endfor %}\n",
    "    </ul>\n",
    "    <h2>3. Strengths</h2>\n",
    "    <ul>\n",
    "      {% for s in strengths %}\n",
    "        <li>{{s}}</li>\n",
    "      {% endfor %}\n",
    "    </ul>\n",
    "    <h2>4. Weaknesses</h2>\n",
    "    <ul>\n",
    "      {% for w in weaknesses %}\n",
    "        <li>{{w}}</li>\n",
    "      {% endfor %}\n",
    "    </ul>\n",
    "    <h2>5. Areas for Future Research & Improvement</h2>\n",
    "    <ul>\n",
    "      {% for f in future %}\n",
    "        <li>{{f}}</li>\n",
    "      {% endfor %}\n",
    "    </ul>\n",
    "    <p><i>Report generated on {{date}}</i></p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "    html = template.render(\n",
    "        det_summary=det_summary,\n",
    "        mc_summary=mc_summary,\n",
    "        strengths=strengths,\n",
    "        weaknesses=weaknesses,\n",
    "        future=future,\n",
    "        date=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Performance report saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load deterministic and MC metrics\n",
    "    det_metrics = pd.read_csv(\"results/backtest_reports/deterministic_metrics.csv\").iloc[0].to_dict()\n",
    "    mc_metrics_df = pd.read_csv(\"results/monte_carlo_sims/monte_carlo_metrics.csv\")\n",
    "    det_summary, mc_summary, strengths, weaknesses, future = summarize_findings(det_metrics, mc_metrics_df)\n",
    "    generate_html_report(det_summary, mc_summary, strengths, weaknesses, future)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
