{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f618a06a",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "- To transform the pre-processed data into meaningful features (alpha factors) that can be fed into machine learning models. \n",
    "- This phase also involves extensive Exploratory Data Analysis (EDA) to understand the characteristics and relationships within your feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a62d36",
   "metadata": {},
   "source": [
    "# Load Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ac9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def load_processed_data(processed_dir=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Load cleaned market data, index constituents, ETF flows, and corporate actions from processed_dir.\n",
    "    Returns a dictionary of DataFrames keyed by file type.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for fname in os.listdir(processed_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            key = fname.replace(\".csv\", \"\")\n",
    "            fpath = os.path.join(processed_dir, fname)\n",
    "            data[key] = load_csv_file(fpath)\n",
    "    # Example access:\n",
    "    # market_data = data.get(\"AAPL_ohlcv\") or similar\n",
    "    # index_data = data.get(\"MSCI_constituents\")\n",
    "    # etf_flows = data.get(\"etf_flows\")\n",
    "    # corporate_actions = data.get(\"corporate_actions\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_processed_data()\n",
    "    for k, v in data.items():\n",
    "        print(f\"{k}:{ v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581d768",
   "metadata": {},
   "source": [
    "# Traditional Alpha Factor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "import pandas as pd\n",
    "from src.features.feature_engineering import (\n",
    "    calculate_momentum,\n",
    "    calculate_volatility,\n",
    "    calculate_on_balance_volume,\n",
    "    apply_scaling\n",
    ")\n",
    "\n",
    "def apply_features_to_market_data(processed_dir=\"data/processed\", output_dir=\"data/features\"):\n",
    "    \"\"\"\n",
    "    Apply feature engineering functions to all market data CSVs in processed_dir.\n",
    "    Save the resulting feature DataFrames to output_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for fname in os.listdir(processed_dir):\n",
    "        if \"ohlcv\" in fname and fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(processed_dir, fname)\n",
    "            df = load_csv_file(fpath)\n",
    "            features = pd.DataFrame(index=df.index)\n",
    "            # Price-based features\n",
    "            features = pd.concat([features, calculate_momentum(df)], axis=1)\n",
    "            features = pd.concat([features, calculate_volatility(df)], axis=1)\n",
    "            # Volume-based features\n",
    "            features = pd.concat([features, calculate_on_balance_volume(df)], axis=1)\n",
    "            # Scaling (optional, can be adjusted)\n",
    "            features = apply_scaling(features, method='standard')\n",
    "            # Save features\n",
    "            out_path = os.path.join(output_dir, fname.replace(\".csv\", \"_features.csv\"))\n",
    "            features.to_csv(out_path, index=False)\n",
    "            print(f\"Saved features to {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_features_to_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e045d8",
   "metadata": {},
   "source": [
    "# Event-Driven Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4329db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_index_rebalance_features(market_df, index_constituents_df, rebalance_dates, ticker_col='ticker', date_col='date'):\n",
    "    \"\"\"\n",
    "    Add features for index rebalances:\n",
    "    - is_upcoming_inclusion: 1 if ticker will be added soon, else 0\n",
    "    - is_upcoming_exclusion: 1 if ticker will be removed soon, else 0\n",
    "    - expected_weight_change: difference in index weight after rebalance\n",
    "    - days_until_rebalance: days until next rebalance\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    index_constituents_df[date_col] = pd.to_datetime(index_constituents_df[date_col])\n",
    "\n",
    "    # Assume rebalance_dates is a sorted list of pd.Timestamp\n",
    "    next_rebalance = np.searchsorted(rebalance_dates, market_df[date_col])\n",
    "    market_df['days_until_rebalance'] = [\n",
    "        (rebalance_dates[i] - d).days if i < len(rebalance_dates) else np.nan\n",
    "        for d, i in zip(market_df[date_col], next_rebalance)\n",
    "    ]\n",
    "    features['days_until_rebalance'] = market_df['days_until_rebalance']\n",
    "\n",
    "    # Inclusion/Exclusion flags and expected weight change\n",
    "    features['is_upcoming_inclusion'] = 0\n",
    "    features['is_upcoming_exclusion'] = 0\n",
    "    features['expected_weight_change'] = 0.0\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col]\n",
    "        # Find current and next constituent status\n",
    "        current = index_constituents_df[\n",
    "            (index_constituents_df[ticker_col] == ticker) &\n",
    "            (index_constituents_df[date_col] <= date)\n",
    "        ].sort_values(date_col).tail(1)\n",
    "        next_ = index_constituents_df[\n",
    "            (index_constituents_df[ticker_col] == ticker) &\n",
    "            (index_constituents_df[date_col] > date)\n",
    "        ].sort_values(date_col).head(1)\n",
    "        # Inclusion\n",
    "        if current.empty and not next_.empty:\n",
    "            features.at[idx, 'is_upcoming_inclusion'] = 1\n",
    "            features.at[idx, 'expected_weight_change'] = next_['weight'].values[0]\n",
    "        # Exclusion\n",
    "        if not current.empty and next_.empty:\n",
    "            features.at[idx, 'is_upcoming_exclusion'] = 1\n",
    "            features.at[idx, 'expected_weight_change'] = -current['weight'].values[0]\n",
    "        # Weight change\n",
    "        if not current.empty and not next_.empty:\n",
    "            features.at[idx, 'expected_weight_change'] = next_['weight'].values[0] - current['weight'].values[0]\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_etf_flow_features(market_df, etf_flows_df, etf_constituents_df, ticker_col='ticker', date_col='date', lag_days=1):\n",
    "    \"\"\"\n",
    "    Estimate passive buying/selling pressure from ETF flows.\n",
    "    - etf_flow_pressure: sum of (ETF flow * stock weight in ETF), lagged if desired\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    etf_flows_df[date_col] = pd.to_datetime(etf_flows_df[date_col])\n",
    "    etf_constituents_df[date_col] = pd.to_datetime(etf_constituents_df[date_col])\n",
    "\n",
    "    features['etf_flow_pressure'] = 0.0\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col] - pd.Timedelta(days=lag_days)\n",
    "        # For each ETF, get flow and constituent weight for this ticker\n",
    "        pressure = 0.0\n",
    "        for etf in etf_flows_df['etf'].unique():\n",
    "            flow_row = etf_flows_df[(etf_flows_df['etf'] == etf) & (etf_flows_df[date_col] == date)]\n",
    "            weight_row = etf_constituents_df[\n",
    "                (etf_constituents_df['etf'] == etf) &\n",
    "                (etf_constituents_df[ticker_col] == ticker) &\n",
    "                (etf_constituents_df[date_col] <= date)\n",
    "            ].sort_values(date_col).tail(1)\n",
    "            if not flow_row.empty and not weight_row.empty:\n",
    "                flow = flow_row['flow_usd'].values[0]\n",
    "                weight = weight_row['weight'].values[0]\n",
    "                pressure += flow * weight\n",
    "        features.at[idx, 'etf_flow_pressure'] = pressure\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_corporate_action_features(market_df, corp_actions_df, ticker_col='ticker', date_col='date', event_types=None, max_days_ahead=30):\n",
    "    \"\"\"\n",
    "    Create binary flags and time-until-event features for upcoming corporate actions.\n",
    "    - For each event type, create a flag and days_until_event feature.\n",
    "    \"\"\"\n",
    "    if event_types is None:\n",
    "        event_types = ['split', 'dividend', 'merger', 'spinoff']\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    corp_actions_df[date_col] = pd.to_datetime(corp_actions_df[date_col])\n",
    "\n",
    "    for event in event_types:\n",
    "        features[f'upcoming_{event}'] = 0\n",
    "        features[f'days_until_{event}'] = np.nan\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col]\n",
    "        for event in event_types:\n",
    "            future_events = corp_actions_df[\n",
    "                (corp_actions_df[ticker_col] == ticker) &\n",
    "                (corp_actions_df['action'] == event) &\n",
    "                (corp_actions_df[date_col] >= date) &\n",
    "                (corp_actions_df[date_col] <= date + pd.Timedelta(days=max_days_ahead))\n",
    "            ]\n",
    "            if not future_events.empty:\n",
    "                soonest = future_events[date_col].min()\n",
    "                features.at[idx, f'upcoming_{event}'] = 1\n",
    "                features.at[idx, f'days_until_{event}'] = (soonest - date).days\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features using tf-quant-finance & QuantLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72649918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Option-implied features using tfqf_pricing_models.py\n",
    "from src.models.tfqf_pricing_models import calculate_implied_volatility\n",
    "\n",
    "def add_option_implied_features(options_df, strikes, expiries, spots, rates):\n",
    "    \"\"\"\n",
    "    Calculate implied volatility, skew, and kurtosis using tf-quant-finance.\n",
    "    Returns a DataFrame with these features.\n",
    "    \"\"\"\n",
    "    # Implied volatility\n",
    "    iv = calculate_implied_volatility(\n",
    "        option_prices=options_df['option_price'],\n",
    "        strikes=strikes,\n",
    "        expiries=expiries,\n",
    "        spots=spots,\n",
    "        rates=rates,\n",
    "        is_call=options_df.get('is_call', True)\n",
    "    ).numpy()\n",
    "    features = pd.DataFrame({'implied_volatility': iv})\n",
    "\n",
    "    # Skew: difference between 25-delta put and 25-delta call IVs (conceptual)\n",
    "    # For demonstration, use percentiles of IV by strike\n",
    "    grouped = options_df.groupby('date')\n",
    "    features['iv_skew'] = grouped['implied_volatility'].transform(\n",
    "        lambda x: np.percentile(x, 90) - np.percentile(x, 10)\n",
    "    )\n",
    "\n",
    "    # Kurtosis: excess kurtosis of IV distribution across strikes\n",
    "    features['iv_kurtosis'] = grouped['implied_volatility'].transform(\n",
    "        lambda x: pd.Series(x).kurt()\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "# Yield curve features using QuantLib via instrument_pricer.py\n",
    "from src.quant_instruments.instrument_pricer import build_yield_curve\n",
    "\n",
    "def add_yield_curve_features(deposits, swaps):\n",
    "    \"\"\"\n",
    "    Build a yield curve and derive slope and curvature features.\n",
    "    Returns a dict with curve, slope, and curvature.\n",
    "    \"\"\"\n",
    "    yield_curve = build_yield_curve(deposits, swaps)\n",
    "    # Example: get rates for 2Y, 5Y, 10Y\n",
    "    import QuantLib as ql\n",
    "    today = ql.Date.todaysDate()\n",
    "    r_2y = yield_curve.zeroRate(ql.Period(\"2Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    r_5y = yield_curve.zeroRate(ql.Period(\"5Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    r_10y = yield_curve.zeroRate(ql.Period(\"10Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    # Slope: 10Y - 2Y\n",
    "    slope = r_10y - r_2y\n",
    "    # Curvature: (2*5Y - 2Y - 10Y)\n",
    "    curvature = 2 * r_5y - r_2y - r_10y\n",
    "    return {\n",
    "        \"yield_curve\": yield_curve,\n",
    "        \"slope\": slope,\n",
    "        \"curvature\": curvature\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aec132",
   "metadata": {},
   "source": [
    "# Alternative Data Proxy Features (Conceptual with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def integrate_sentiment_features(market_df, sentiment_df, lookback=3):\n",
    "    \"\"\"\n",
    "    Merge rolling mean sentiment scores into market data.\n",
    "    \"\"\"\n",
    "    sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "    sentiment_df = sentiment_df.sort_values(['ticker', 'date'])\n",
    "    sentiment_df['sentiment_rolling_mean'] = (\n",
    "        sentiment_df.groupby('ticker')['sentiment_score']\n",
    "        .transform(lambda x: x.rolling(lookback, min_periods=1).mean())\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        market_df,\n",
    "        sentiment_df[['date', 'ticker', 'sentiment_rolling_mean']],\n",
    "        on=['date', 'ticker'],\n",
    "        how='left'\n",
    "    )\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f545054",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3297c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def plot_feature_distributions(df, feature_cols, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot histograms for each feature.\n",
    "    \"\"\"\n",
    "    for col in feature_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=50)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, f\"dist_{col}.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def plot_feature_correlations(df, feature_cols, target_col=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix and feature-target correlations.\n",
    "    \"\"\"\n",
    "    corr = df[feature_cols].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, \"feature_corr_matrix.png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_corr = df[feature_cols + [target_col]].corr()[target_col].drop(target_col)\n",
    "        print(\"Feature-Target Correlations:\")\n",
    "        print(target_corr.sort_values(ascending=False))\n",
    "\n",
    "def plot_time_series(df, feature_cols, date_col='date', output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot time series of features.\n",
    "    \"\"\"\n",
    "    for col in feature_cols:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df[date_col], df[col])\n",
    "        plt.title(f\"{col} over time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(col)\n",
    "        if output_dir:\n",
    "            plt.savefig(os.path.join(output_dir, f\"ts_{col}.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def cross_sectional_analysis(df, feature_cols, ticker_col='ticker', date_col='date', snapshot_date=None):\n",
    "    \"\"\"\n",
    "    Analyze feature values across tickers at a given date.\n",
    "    \"\"\"\n",
    "    if snapshot_date is None:\n",
    "        snapshot_date = df[date_col].max()\n",
    "    snapshot = df[df[date_col] == snapshot_date]\n",
    "    print(f\"Cross-sectional snapshot for {snapshot_date}:\")\n",
    "    print(snapshot[[ticker_col] + feature_cols].describe())\n",
    "\n",
    "def preliminary_feature_importance(df, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Estimate feature importance using correlation and a simple tree model.\n",
    "    \"\"\"\n",
    "    # Correlation\n",
    "    corrs = df[feature_cols + [target_col]].corr()[target_col].drop(target_col)\n",
    "    print(\"Correlation-based importance:\")\n",
    "    print(corrs.sort_values(ascending=False))\n",
    "\n",
    "    # Tree-based (RandomForest)\n",
    "    try:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        X = df[feature_cols].fillna(0)\n",
    "        y = df[target_col].fillna(0)\n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        importances = pd.Series(model.feature_importances_, index=feature_cols)\n",
    "        print(\"RandomForest feature importances:\")\n",
    "        print(importances.sort_values(ascending=False))\n",
    "    except ImportError:\n",
    "        print(\"scikit-learn not installed, skipping tree-based importance.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    features_path = \"data/features/AAPL_ohlcv_features.csv\"\n",
    "    df = load_csv_file(features_path)\n",
    "    # Add your target variable (e.g., future returns) to df before running EDA\n",
    "    feature_cols = [col for col in df.columns if \"momentum\" in col or \"volatility\" in col or \"volume\" in col]\n",
    "    target_col = \"future_return\"  # Replace with your actual target column\n",
    "\n",
    "    plot_feature_distributions(df, feature_cols)\n",
    "    plot_feature_correlations(df, feature_cols, target_col=target_col)\n",
    "    plot_time_series(df, feature_cols, date_col='date')\n",
    "    cross_sectional_analysis(df, feature_cols, ticker_col='ticker', date_col='date')\n",
    "    preliminary_feature_importance(df, feature_cols, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38887d10",
   "metadata": {},
   "source": [
    "# Target Variable Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_forward_returns(df, price_col='adj_close', horizons=[1, 5, 20]):\n",
    "    \"\"\"\n",
    "    Calculate forward returns for given horizons.\n",
    "    :param df: DataFrame with price_col\n",
    "    :param price_col: str, column name for adjusted close price\n",
    "    :param horizons: list of int, forward return horizons in days\n",
    "    :return: DataFrame with new columns for each forward return\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for h in horizons:\n",
    "        # Forward return: (future_price - today_price) / today_price\n",
    "        df[f'forward_return_{h}d'] = (df[price_col].shift(-h) - df[price_col]) / df[price_col]\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = calculate_forward_returns(df, price_col='adj_close', horizons=[1, 5, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9e500",
   "metadata": {},
   "source": [
    "# Data Splitting & Feature Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18919804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def time_series_split_save(df, date_col='date', train_frac=0.6, val_frac=0.2, test_frac=0.2, output_dir=\"data/processed\", base_filename=\"final_features\"):\n",
    "    \"\"\"\n",
    "    Chronologically split the dataset into train/val/test and save as Parquet.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(date_col)\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_frac)\n",
    "    val_end = train_end + int(n * val_frac)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train.to_parquet(os.path.join(output_dir, f\"{base_filename}_train.parquet\"), index=False)\n",
    "    val.to_parquet(os.path.join(output_dir, f\"{base_filename}_val.parquet\"), index=False)\n",
    "    test.to_parquet(os.path.join(output_dir, f\"{base_filename}_test.parquet\"), index=False)\n",
    "    print(f\"Saved train/val/test splits to {output_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_parquet(\"data/processed/combined_features.parquet\")\n",
    "# time_series_split_save(df, date_col='date', output_dir=\"data/processed\", base_filename=\"final_features\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
