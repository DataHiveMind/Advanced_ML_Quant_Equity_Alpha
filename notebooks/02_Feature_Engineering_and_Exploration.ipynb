{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f618a06a",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "- To transform the pre-processed data into meaningful features (alpha factors) that can be fed into machine learning models. \n",
    "- This phase also involves extensive Exploratory Data Analysis (EDA) to understand the characteristics and relationships within your feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a62d36",
   "metadata": {},
   "source": [
    "# Load Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ac9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "\n",
    "def load_processed_data(processed_dir=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Load cleaned market data, index constituents, ETF flows, and corporate actions from processed_dir.\n",
    "    Returns a dictionary of DataFrames keyed by file type.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for fname in os.listdir(processed_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            key = fname.replace(\".csv\", \"\")\n",
    "            fpath = os.path.join(processed_dir, fname)\n",
    "            data[key] = load_csv_file(fpath)\n",
    "    # Example access:\n",
    "    # market_data = data.get(\"AAPL_ohlcv\") or similar\n",
    "    # index_data = data.get(\"MSCI_constituents\")\n",
    "    # etf_flows = data.get(\"etf_flows\")\n",
    "    # corporate_actions = data.get(\"corporate_actions\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_processed_data()\n",
    "    for k, v in data.items():\n",
    "        print(f\"{k}:{ v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581d768",
   "metadata": {},
   "source": [
    "# Traditional Alpha Factor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data_ingestion.data_loader import load_csv_file\n",
    "import pandas as pd\n",
    "from src.features.feature_engineering import (\n",
    "    calculate_momentum,\n",
    "    calculate_volatility,\n",
    "    calculate_on_balance_volume,\n",
    "    apply_scaling\n",
    ")\n",
    "\n",
    "def apply_features_to_market_data(processed_dir=\"data/processed\", output_dir=\"data/features\"):\n",
    "    \"\"\"\n",
    "    Apply feature engineering functions to all market data CSVs in processed_dir.\n",
    "    Save the resulting feature DataFrames to output_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for fname in os.listdir(processed_dir):\n",
    "        if \"ohlcv\" in fname and fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(processed_dir, fname)\n",
    "            df = load_csv_file(fpath)\n",
    "            features = pd.DataFrame(index=df.index)\n",
    "            # Price-based features\n",
    "            features = pd.concat([features, calculate_momentum(df)], axis=1)\n",
    "            features = pd.concat([features, calculate_volatility(df)], axis=1)\n",
    "            # Volume-based features\n",
    "            features = pd.concat([features, calculate_on_balance_volume(df)], axis=1)\n",
    "            # Scaling (optional, can be adjusted)\n",
    "            features = apply_scaling(features, method='standard')\n",
    "            # Save features\n",
    "            out_path = os.path.join(output_dir, fname.replace(\".csv\", \"_features.csv\"))\n",
    "            features.to_csv(out_path, index=False)\n",
    "            print(f\"Saved features to {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_features_to_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e045d8",
   "metadata": {},
   "source": [
    "# Event-Driven Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4329db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_index_rebalance_features(market_df, index_constituents_df, rebalance_dates, ticker_col='ticker', date_col='date'):\n",
    "    \"\"\"\n",
    "    Add features for index rebalances:\n",
    "    - is_upcoming_inclusion: 1 if ticker will be added soon, else 0\n",
    "    - is_upcoming_exclusion: 1 if ticker will be removed soon, else 0\n",
    "    - expected_weight_change: difference in index weight after rebalance\n",
    "    - days_until_rebalance: days until next rebalance\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    index_constituents_df[date_col] = pd.to_datetime(index_constituents_df[date_col])\n",
    "\n",
    "    # Assume rebalance_dates is a sorted list of pd.Timestamp\n",
    "    next_rebalance = np.searchsorted(rebalance_dates, market_df[date_col])\n",
    "    market_df['days_until_rebalance'] = [\n",
    "        (rebalance_dates[i] - d).days if i < len(rebalance_dates) else np.nan\n",
    "        for d, i in zip(market_df[date_col], next_rebalance)\n",
    "    ]\n",
    "    features['days_until_rebalance'] = market_df['days_until_rebalance']\n",
    "\n",
    "    # Inclusion/Exclusion flags and expected weight change\n",
    "    features['is_upcoming_inclusion'] = 0\n",
    "    features['is_upcoming_exclusion'] = 0\n",
    "    features['expected_weight_change'] = 0.0\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col]\n",
    "        # Find current and next constituent status\n",
    "        current = index_constituents_df[\n",
    "            (index_constituents_df[ticker_col] == ticker) &\n",
    "            (index_constituents_df[date_col] <= date)\n",
    "        ].sort_values(date_col).tail(1)\n",
    "        next_ = index_constituents_df[\n",
    "            (index_constituents_df[ticker_col] == ticker) &\n",
    "            (index_constituents_df[date_col] > date)\n",
    "        ].sort_values(date_col).head(1)\n",
    "        # Inclusion\n",
    "        if current.empty and not next_.empty:\n",
    "            features.at[idx, 'is_upcoming_inclusion'] = 1\n",
    "            features.at[idx, 'expected_weight_change'] = next_['weight'].values[0]\n",
    "        # Exclusion\n",
    "        if not current.empty and next_.empty:\n",
    "            features.at[idx, 'is_upcoming_exclusion'] = 1\n",
    "            features.at[idx, 'expected_weight_change'] = -current['weight'].values[0]\n",
    "        # Weight change\n",
    "        if not current.empty and not next_.empty:\n",
    "            features.at[idx, 'expected_weight_change'] = next_['weight'].values[0] - current['weight'].values[0]\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_etf_flow_features(market_df, etf_flows_df, etf_constituents_df, ticker_col='ticker', date_col='date', lag_days=1):\n",
    "    \"\"\"\n",
    "    Estimate passive buying/selling pressure from ETF flows.\n",
    "    - etf_flow_pressure: sum of (ETF flow * stock weight in ETF), lagged if desired\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    etf_flows_df[date_col] = pd.to_datetime(etf_flows_df[date_col])\n",
    "    etf_constituents_df[date_col] = pd.to_datetime(etf_constituents_df[date_col])\n",
    "\n",
    "    features['etf_flow_pressure'] = 0.0\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col] - pd.Timedelta(days=lag_days)\n",
    "        # For each ETF, get flow and constituent weight for this ticker\n",
    "        pressure = 0.0\n",
    "        for etf in etf_flows_df['etf'].unique():\n",
    "            flow_row = etf_flows_df[(etf_flows_df['etf'] == etf) & (etf_flows_df[date_col] == date)]\n",
    "            weight_row = etf_constituents_df[\n",
    "                (etf_constituents_df['etf'] == etf) &\n",
    "                (etf_constituents_df[ticker_col] == ticker) &\n",
    "                (etf_constituents_df[date_col] <= date)\n",
    "            ].sort_values(date_col).tail(1)\n",
    "            if not flow_row.empty and not weight_row.empty:\n",
    "                flow = flow_row['flow_usd'].values[0]\n",
    "                weight = weight_row['weight'].values[0]\n",
    "                pressure += flow * weight\n",
    "        features.at[idx, 'etf_flow_pressure'] = pressure\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_corporate_action_features(market_df, corp_actions_df, ticker_col='ticker', date_col='date', event_types=None, max_days_ahead=30):\n",
    "    \"\"\"\n",
    "    Create binary flags and time-until-event features for upcoming corporate actions.\n",
    "    - For each event type, create a flag and days_until_event feature.\n",
    "    \"\"\"\n",
    "    if event_types is None:\n",
    "        event_types = ['split', 'dividend', 'merger', 'spinoff']\n",
    "    features = pd.DataFrame(index=market_df.index)\n",
    "    market_df = market_df.copy()\n",
    "    market_df[date_col] = pd.to_datetime(market_df[date_col])\n",
    "    corp_actions_df[date_col] = pd.to_datetime(corp_actions_df[date_col])\n",
    "\n",
    "    for event in event_types:\n",
    "        features[f'upcoming_{event}'] = 0\n",
    "        features[f'days_until_{event}'] = np.nan\n",
    "\n",
    "    for idx, row in market_df.iterrows():\n",
    "        ticker = row[ticker_col]\n",
    "        date = row[date_col]\n",
    "        for event in event_types:\n",
    "            future_events = corp_actions_df[\n",
    "                (corp_actions_df[ticker_col] == ticker) &\n",
    "                (corp_actions_df['action'] == event) &\n",
    "                (corp_actions_df[date_col] >= date) &\n",
    "                (corp_actions_df[date_col] <= date + pd.Timedelta(days=max_days_ahead))\n",
    "            ]\n",
    "            if not future_events.empty:\n",
    "                soonest = future_events[date_col].min()\n",
    "                features.at[idx, f'upcoming_{event}'] = 1\n",
    "                features.at[idx, f'days_until_{event}'] = (soonest - date).days\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features using tf-quant-finance & QuantLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72649918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Option-implied features using tfqf_pricing_models.py\n",
    "from src.models.tfqf_pricing_models import calculate_implied_volatility\n",
    "\n",
    "def add_option_implied_features(options_df, strikes, expiries, spots, rates):\n",
    "    \"\"\"\n",
    "    Calculate implied volatility, skew, and kurtosis using tf-quant-finance.\n",
    "    Returns a DataFrame with these features.\n",
    "    \"\"\"\n",
    "    # Implied volatility\n",
    "    iv = calculate_implied_volatility(\n",
    "        option_prices=options_df['option_price'],\n",
    "        strikes=strikes,\n",
    "        expiries=expiries,\n",
    "        spots=spots,\n",
    "        rates=rates,\n",
    "        is_call=options_df.get('is_call', True)\n",
    "    ).numpy()\n",
    "    features = pd.DataFrame({'implied_volatility': iv})\n",
    "\n",
    "    # Skew: difference between 25-delta put and 25-delta call IVs (conceptual)\n",
    "    # For demonstration, use percentiles of IV by strike\n",
    "    grouped = options_df.groupby('date')\n",
    "    features['iv_skew'] = grouped['implied_volatility'].transform(\n",
    "        lambda x: np.percentile(x, 90) - np.percentile(x, 10)\n",
    "    )\n",
    "\n",
    "    # Kurtosis: excess kurtosis of IV distribution across strikes\n",
    "    features['iv_kurtosis'] = grouped['implied_volatility'].transform(\n",
    "        lambda x: pd.Series(x).kurt()\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "# Yield curve features using QuantLib via instrument_pricer.py\n",
    "from src.quant_instruments.instrument_pricer import build_yield_curve\n",
    "\n",
    "def add_yield_curve_features(deposits, swaps):\n",
    "    \"\"\"\n",
    "    Build a yield curve and derive slope and curvature features.\n",
    "    Returns a dict with curve, slope, and curvature.\n",
    "    \"\"\"\n",
    "    yield_curve = build_yield_curve(deposits, swaps)\n",
    "    # Example: get rates for 2Y, 5Y, 10Y\n",
    "    import QuantLib as ql\n",
    "    today = ql.Date.todaysDate()\n",
    "    r_2y = yield_curve.zeroRate(ql.Period(\"2Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    r_5y = yield_curve.zeroRate(ql.Period(\"5Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    r_10y = yield_curve.zeroRate(ql.Period(\"10Y\"), ql.Actual365Fixed(), ql.Compounded).rate()\n",
    "    # Slope: 10Y - 2Y\n",
    "    slope = r_10y - r_2y\n",
    "    # Curvature: (2*5Y - 2Y - 10Y)\n",
    "    curvature = 2 * r_5y - r_2y - r_10y\n",
    "    return {\n",
    "        \"yield_curve\": yield_curve,\n",
    "        \"slope\": slope,\n",
    "        \"curvature\": curvature\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aec132",
   "metadata": {},
   "source": [
    "# Alternative Data Proxy Features (Conceptual with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def integrate_sentiment_features(market_df, sentiment_df, lookback=3):\n",
    "    \"\"\"\n",
    "    Merge rolling mean sentiment scores into market data.\n",
    "    \"\"\"\n",
    "    sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "    sentiment_df = sentiment_df.sort_values(['ticker', 'date'])\n",
    "    sentiment_df['sentiment_rolling_mean'] = (\n",
    "        sentiment_df.groupby('ticker')['sentiment_score']\n",
    "        .transform(lambda x: x.rolling(lookback, min_periods=1).mean())\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        market_df,\n",
    "        sentiment_df[['date', 'ticker', 'sentiment_rolling_mean']],\n",
    "        on=['date', 'ticker'],\n",
    "        how='left'\n",
    "    )\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f545054",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3297c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38887d10",
   "metadata": {},
   "source": [
    "# Target Variable Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487f3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3f9e500",
   "metadata": {},
   "source": [
    "# Data Splitting & Feature Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18919804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
